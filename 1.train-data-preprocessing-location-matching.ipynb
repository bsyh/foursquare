{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom geopy.geocoders import Nominatim\n\nimport re\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords, wordnet as wn\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\nimport Levenshtein as lev\nimport math\nfrom collections import Counter\n\nfrom pickle import dump, load\nimport time\nfrom sklearn.neighbors import BallTree\n\n\nimport itertools\nfrom tqdm.auto import tqdm\ntqdm.pandas()\nimport gc\n","metadata":{"execution":{"iopub.status.busy":"2022-07-04T16:10:54.545604Z","iopub.execute_input":"2022-07-04T16:10:54.546486Z","iopub.status.idle":"2022-07-04T16:10:56.059480Z","shell.execute_reply.started":"2022-07-04T16:10:54.546317Z","shell.execute_reply":"2022-07-04T16:10:56.058496Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T16:10:56.063648Z","iopub.execute_input":"2022-07-04T16:10:56.064964Z","iopub.status.idle":"2022-07-04T16:10:56.069811Z","shell.execute_reply.started":"2022-07-04T16:10:56.064918Z","shell.execute_reply":"2022-07-04T16:10:56.068835Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"pairs = pd.read_pickle('../input/fourpoints-location-matching/train_pairs_raw.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-07-04T16:10:56.072139Z","iopub.execute_input":"2022-07-04T16:10:56.072950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs = pairs.sample(frac=0.1, random_state=1, ignore_index=True)\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs_sample = pd.read_csv('../input/foursquare-location-matching/pairs.csv').iloc[0:2,:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# change original data type\ndel pairs['index']\ndtype_dict = pairs_sample.dtypes.apply(lambda x: x.name).to_dict()\ndel pairs_sample\ngc.collect()\npairs = pairs.astype(dtype_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reduce memory","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in tqdm(df.columns):\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs = reduce_mem_usage(pairs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train = pd.read_csv('../input/foursquare-location-matching/train.csv')\n#train = reduce_mem_usage(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for col in tqdm(['latitude_1', 'longitude_1', 'latitude_2', 'longitude_2']):\n    pairs[col] = pairs[col].astype('float16')","metadata":{"execution":{"iopub.status.busy":"2022-07-03T14:38:13.936612Z","iopub.status.idle":"2022-07-03T14:38:13.936952Z","shell.execute_reply.started":"2022-07-03T14:38:13.936801Z","shell.execute_reply":"2022-07-03T14:38:13.936817Z"}}},{"cell_type":"code","source":"word_columns = ['name_1','address_1','city_1','state_1','url_1','categories_1','name_2','address_2','city_2','state_2','url_2','categories_2']\nfor col in tqdm(word_columns):\n    pairs[col] = pairs[col].astype('object')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing values generate new cols\nmissing_list = ['url_1','url_2','phone_1','phone_2','address_1','address_2','city_1','city_2','zip_1','zip_2']\nfor col in tqdm(missing_list):\n    pairs[f\"{col}_missing\"] = pairs[col].notnull().astype('int8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# impute missing values\ncat_col = pairs.select_dtypes(include = ['object']).columns\npairs[cat_col] = pairs[cat_col].fillna('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data preprocessing & Feature transformation:\n1. location (latitude, longtitude): finding the distance between two variables\n2. word preprocessing: remove url symbol, stop words removal","metadata":{}},{"cell_type":"code","source":"# 1. location\ndef distance(lat1, lon1, lat2, lon2):\n    R = 6373.0\n    d_lon = lon2 - lon1\n    d_lat = lat2 - lat1\n    a = (np.sin(d_lat/2)) ** 2 + np.cos(lat1) * np.cos(lat2) * (np.sin(d_lon/2)) ** 2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    distance = R * c\n    return distance\n\n# lowercase\ndef lower(df, cols):\n    for col in tqdm(cols):\n        df[col] = df[col].progress_apply(lambda x: x.lower())\n    return df\n\n# number removing\ndef num_remove(df, cols):\n    for col in tqdm(cols):\n        df[col] = df[col].progress_apply(lambda x: re.sub(r'\\d+', '', x))\n    return df\n\n# punctuation removal\ndef punc_remove(df,cols):\n    for col in tqdm(cols):\n        df[col] = df[col].progress_apply(lambda x: x.translate(str.maketrans(\"\",\"\", string.punctuation)))\n    return df\n\n# white spaces removal\ndef space_remove(df,cols):\n    for col in tqdm(cols):\n        df[col] = df[col].progress_apply(lambda x: x.strip())\n    return df\n\ndef preprocess(df,cols):\n    df[cols] = num_remove(df,cols)[cols]\n    df[cols] = punc_remove(df,cols)[cols]\n    df[cols] = space_remove(df,cols)[cols]\n    return df\n\n# remove url\ndef remove_URL(df,cols):\n    # cols = [\"url_1\",\"url_2\"]\n    df[cols] = df[cols].fillna('')\n    for i in tqdm(cols):\n        df[i] = df[i].str.replace('http://', '')\n        df[i] = df[i].str.replace('https://', '')\n        df[i] = df[i].str.replace('http:', '')\n        df[i] = df[i].str.replace('https:', '')\n        df[i] = df[i].str.replace('http', '')\n        df[i] = df[i].str.replace('https', '')\n        df[i] = df[i].str.replace('www.', '')\n        df[i] = df[i].str.replace('www', '')\n        df[i] = df[i].progress_apply(lambda x: re.sub('\\W', \"\", x))\n    return df\n\n# stop words removal\n\ndef list_to_string(lis):\n    string = ''\n    for i in tqdm(lis):\n        string += i\n        string += ' '\n    return string[:-1]\n\ndef stop(string):\n    stops = set(stopwords.words('english'))\n    tokens = word_tokenize(string)\n    result = [i for i in tqdm(tokens) if not i in stops]\n    return result\n    \ndef stop_remove(df,cols):\n    stops = set(stopwords.words('english'))\n    for col in tqdm(cols):\n        df[col] = df[col].progress_apply(lambda x: ' '.join([word for word in x.split() if word not in stops]))\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lowercase_cols = ['name_1','address_1','city_1','state_1','url_1','categories_1','name_2','address_2','city_2','state_2','url_2','categories_2']\npreprocess_cols = ['name_1','address_1','name_2','address_2','url_1','url_2']\nurl_columns = ['url_1','url_2']\npairs['distance'] = distance(pairs.latitude_1,pairs.longitude_1,pairs.latitude_2,pairs.longitude_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs[lowercase_cols].info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs[lowercase_cols] = lower(pairs, lowercase_cols)[lowercase_cols]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs[preprocess_cols] = preprocess(pairs, preprocess_cols)[preprocess_cols]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs[url_columns] = remove_URL(pairs, url_columns)[url_columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs[preprocess_cols] = stop_remove(pairs, preprocess_cols)[preprocess_cols]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"text similarity: fuzzy, cosine similarity","metadata":{}},{"cell_type":"code","source":"def fuzzy_similarity(df, cols_1, cols_2):\n    # length for cols_1 and cols_2 must be the same.\n    for i in tqdm(range(len(cols_1))):\n        df[f\"{cols_1[i]}_fuzzy\"] = df.progress_apply(lambda x: lev.ratio(x[cols_1[i]],x[cols_2[i]]), axis = 1)\n    return df    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_1 = ['name_1','address_1','categories_1']\ncol_2 = ['name_2','address_2','categories_2']\npairs = fuzzy_similarity(pairs, col_1, col_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U sentence_transformers\nfrom sentence_transformers import SentenceTransformer, util\nmodel = SentenceTransformer('stsb-roberta-large')\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_1 = ['name_1','address_1','categories_1']\ncol_2 = ['name_2','address_2','categories_2']\ndef cos_sim(text_list_1,text_list_2):\n    model = SentenceTransformer('stsb-roberta-large')\n    #model = SentenceTransformer('all-MiniLM-L6-v2')\n    start_time = time.time()\n    pool = model.start_multi_process_pool()\n    embedding1 = model.encode_multi_process(text_list_1, pool, batch_size =1024)\n    embedding2 = model.encode(text_list_2, convert_to_tensor=True)\n    total_similarity = util.cos_sim(embeddings1, embeddings2)\n    model.stop_multi_process_pool(pool)\n    return np.array(total_similarity).diagonal()\n\ndef cos_similarity(df, cols_1, cols_2):\n    # length for cols_1 and cols_2 must be the same.\n    for i in tqdm(range(len(cols_1))):\n        df[f\"{cols_1[i]}_cos_sim\"] = cos_sim(df[cols_1[i]].tolist(),df[cols_2[i]].tolist())\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Two lists of sentences\nsentences1 = ['The cat sits outside','The cat sits outside',\n             'A man is playing guitar',\n             'The new movie is awesome','The dog plays in the garden',\n              'A woman watches TV',\n              'The new movie is so great']\n\nsentences2 = ['The dog plays in the garden',\n              'A woman watches TV',\n              'The new movie is so great']\n\n#Compute embedding for both lists\nembeddings1 = model.encode(sentences1, convert_to_tensor=False)\n'''\nembeddings2 = model.encode(sentences2, convert_to_tensor=True)\n#Compute cosine-similarities\ncosine_scores = util.cos_sim(embeddings1, embeddings2)\n\n#Output the pairs with their score\nfor i in range(len(sentences1)):\n    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))\n'''\nembeddings1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs = cos_similarity(pairs, col_1, col_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs.to_csv(\"train_pairs.csv\", index=False)\npairs.to_pickle('./train_pairs.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"--- %s seconds ---\" % (time.time() - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"pairs_sample = pairs.iloc[:2,:]\ndtype_dict = pairs_sample.dtypes.apply(lambda x: x.name).to_dict()\ndel pairs_sample\ndtype_dict","metadata":{}},{"cell_type":"code","source":"pairs.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# download data\n<a href=\"train_pairs.csv\"> train_pairs </a>\n\n<a href=\"./train_pairs.pkl\"> train_pairs pickle </a>","metadata":{}},{"cell_type":"markdown","source":"# Appendix\n\nread large data: https://www.kaggle.com/code/rohanrao/tutorial-on-reading-large-datasets/notebook\n\nread data faster: https://towardsdatascience.com/%EF%B8%8F-load-the-same-csv-file-10x-times-faster-and-with-10x-less-memory-%EF%B8%8F-e93b485086c7","metadata":{}}]}